# A Brief Guide to Classification

> **ç»™ LHM Summer Program AIè¯¾ç¨‹å°ä¼™ä¼´çš„é€Ÿæ•‘æŒ‡å—**
> Author: [*Lifan Lin*](https://flammingfrost.github.io/tech-blog/)
> Date: *2023-08-15*
> ä¸ºäº†é¿å…è¯­è¨€ä¸Šçš„éšœç¢,è¿˜æ˜¯ç”¨ä¸­æ–‡å†™å§

---

## 1. ä»€ä¹ˆæ˜¯åˆ†ç±»é—®é¢˜

åˆ†ç±»é—®é¢˜(classification)æ˜¯åŒºåˆ«ä¸Žå›žå½’é—®é¢˜(regression)çš„ä¸€ç±»é—®é¢˜. æœ€ç›´æŽ¥çš„åŒºåˆ«æ˜¯æ¨¡åž‹çš„è¾“å‡º:
- Regression: ä¸€ä¸ªè¿žç»­çš„æ•°å€¼. æ¯”å¦‚é¢„æµ‹æˆ¿ä»·, é¢„æµ‹è‚¡ä»·ç­‰ç­‰. æ•°å€¼çš„èŒƒå›´å¾€å¾€æ˜¯å®žæ•°åŸŸ.
- Classification: ä¸€ä¸ªç¦»æ•£çš„æ•°å€¼. æ¯”å¦‚é¢„æµ‹ä¸€ä¸ªå›¾ç‰‡æ˜¯çŒ«è¿˜æ˜¯ç‹—, é¢„æµ‹ä¸€ä¸ªäººçš„æ€§åˆ«ç­‰ç­‰. æ•°å€¼çš„èŒƒå›´å¾€å¾€æ˜¯æœ‰é™çš„(ç±»åˆ«).

ç”±äºŽè¾“å‡ºçš„æ•°å€¼ç±»åž‹æœ‰æ‰€åŒºåˆ«, æ‰€é‡‡å–çš„æ–¹æ³•ä¹Ÿæœ‰æ‰€ä¸åŒ. 

> æ³¨æ„: é€»è¾‘æ–¯è’‚å›žå½’(Logistic Regression)æ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•, è€Œä¸æ˜¯å›žå½’ç®—æ³•.

## 2. æŸå¤±å‡½æ•°: äº¤å‰ç†µ(Cross Entropy)

### 2.1 ç‹¬çƒ­ç¼–ç (One-hot Encoding)

ä»¥æ—¥å¸¸çš„ä¹ æƒ¯, æˆ‘ä»¬ä¹Ÿè®¸ä¼šä½¿ç”¨ä¸åŒçš„æ•°å­—æ¥è¡¨è¾¾ä¸åŒçš„åˆ†ç±» (*æ¯”å¦‚ä½¿ç”¨1è¡¨ç¤ºè¥¿ç“œ, 2è¡¨ç¤ºè‹¹æžœ, 3è¡¨ç¤ºé¦™è•‰ ç­‰ç­‰*), ä½†è¿™å¯¹äºŽæˆ‘ä»¬çš„æ¨¡åž‹æ˜¯ä¸åˆç†çš„. å› ä¸ºè¿™æ ·çš„æ•°å­—ä¹‹é—´å­˜åœ¨ç€å¤§å°å…³ç³», è€Œå®žé™…ä¸Šè¿™äº›æ•°å­—åªæ˜¯ç”¨æ¥åŒºåˆ†ä¸åŒçš„ç±»åˆ«è€Œå·². ~~å³ä½¿æ˜¯æœ‰åºçš„ç¦»æ•£å˜é‡(å¦‚æŽ’å), ä¹Ÿä¸åº”è¯¥ç›´æŽ¥ä½¿ç”¨æ•°å­—æ¥è¡¨ç¤º.(å› ä¸ºåŒ…å«äº†ç­‰å·®è·çš„å‡è®¾)~~

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåˆ†ç±»å‘é‡, å¯ä»¥è¿™æ ·è½¬æ¢:
åŽŸå§‹å‘é‡x:
```python
x = [1, 2, 0, 1, 1, 2, 0, 0] #å…±æœ‰ä¸‰ç±»
x = np.array(x)
```
å°†ä»–è½¬æ¢ä½ç‹¬çƒ­ç¼–ç :
```python
num_classes = 3
x_one_hot = np.zeros((x.shape[0], num_classes))
for i in range(num_classes):
    x_one_hot[x == i, i] = 1

print(x_one_hot)
```
è¾“å‡º:
```python
x_one_hot = 

[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]
 [1. 0. 0.]
 [1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]
 [0. 0. 1.]]
```

## 2.2 äº¤å‰ç†µ(Cross Entropy)

äº¤å‰ç†µæ˜¯ä¸€ç§æŸå¤±å‡½æ•°, ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚. äº¤å‰ç†µè¶Š**å°**, ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒè¶Š**æŽ¥è¿‘**. æˆ‘ä»¬å¸Œæœ›æ¨¡åž‹çš„é¢„æµ‹åˆ†ç±»å’ŒçœŸå®žåˆ†ç±»å°½å¯èƒ½çš„æŽ¥è¿‘, å› æ­¤æˆ‘ä»¬å¸Œæœ›äº¤å‰ç†µè¶Šå°è¶Šå¥½.
äº¤å‰ç†µçš„å…¬å¼å¦‚ä¸‹:
$$
H(p, q) = -\sum_{x}^{\text{n\_class}}p(x)log(q(x))
$$
æ³¨æ„: è¿™é‡Œçš„$p(x)$æ˜¯çœŸå®žçš„æ¦‚çŽ‡åˆ†å¸ƒ, $q(x)$æ˜¯æ¨¡åž‹çš„é¢„æµ‹æ¦‚çŽ‡åˆ†å¸ƒ.
ç”¨$y$è¡¨ç¤ºçœŸå®žçš„åˆ†ç±», ç”¨$\hat{y}$è¡¨ç¤ºæ¨¡åž‹çš„é¢„æµ‹åˆ†ç±», äº¤å‰ç†µå¯ä»¥å†™æˆ:
$$
H(y, \hat{y}) = -\sum_{x}^{\text{n\_class}}y(x)log(\hat{y}(x))
$$
å¦‚æžœåˆ†ç±»åªæœ‰ä¸¤ç±», é‚£ä¹ˆäº¤å‰ç†µå¯ä»¥ç®€åŒ–ä¸º:
$$
H(y, \hat{y}) = -ylog(\hat{y}) - (1-y)log(1-\hat{y})
$$
å…¶ä¸­$y$æ˜¯çœŸå®žçš„åˆ†ç±», $\hat{y}$æ˜¯æ¨¡åž‹çš„é¢„æµ‹åˆ†ç±».

## 2.3 Sigmodå‡½æ•°
> è¿™çŽ©æ„æˆ‘çœŸä¸çŸ¥é“æœ‰ä¸­æ–‡åå«ä»€ä¹ˆ, æ‰€ä»¥å°±ç”¨è‹±æ–‡åäº†.

### ä¸ºä»€ä¹ˆéœ€è¦Sigmodå‡½æ•°

å½“æˆ‘ä½¿ç”¨çº¿æ€§æ¨¡åž‹æ¥é¢„æµ‹åˆ†ç±», æˆ‘ä»¬éœ€è¦é¢„æµ‹å„ä¸ªåˆ†ç±»çš„æ¦‚çŽ‡. ç„¶è€Œçº¿æ€§æ¨¡åž‹çš„è¾“å‡ºæ˜¯å®žæ•°åŸŸçš„, å› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°å°†å®žæ•°åŸŸçš„å€¼æ˜ å°„åˆ°(0, 1)ä¹‹é—´.

åœ¨é€»è¾‘æ–¯è’‚å›žå½’ä¸­,æˆ‘ä»¬ä½¿ç”¨äº†sigmoidå‡½æ•°, å› ä¸ºä»–å…·æœ‰å¾ˆå¥½çš„æ€§è´¨. å…·ä½“å¦‚ä¸‹:
1. ä»–çš„å€¼åŸŸåœ¨(0, 1)ä¹‹é—´, å› æ­¤å¯ä»¥ç”¨æ¥è¡¨ç¤ºæ¦‚çŽ‡.
sigmoidå‡½æ•°çš„å…¬å¼å¦‚ä¸‹:
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
ä»–å°†å®žæ•°åŸŸçš„å€¼æ˜ å°„åˆ°(0, 1)ä¹‹é—´.

2. ä»–æ˜¯å•è°ƒé€’å¢žçš„, å› æ­¤ä»–çš„å€¼è¶Šå¤§, æ¦‚çŽ‡è¶Šå¤§.
3. ä»–æ˜¯å¯å¾®çš„, è€Œä¸”æ±‚å¯¼éžå¸¸ç®€å•.
å¯¹äºŒåˆ†ç±»å˜é‡$x\in\{0, 1\}$, sigmoidå‡½æ•°çš„å¯¼æ•°å¦‚ä¸‹:
$$
\sigma'(x) = \sigma(x)(1-\sigma(x))
$$
å¯¹äºŽå¤šåˆ†ç±»å˜é‡$x\in\{0, 1, ..., n\}$, ç”¨ç‹¬çƒ­ç¼–ç (one-hot)è¡¨ç¤ºæ—¶, sigmoidå‡½æ•°å¦‚ä¸‹:
$$
\sigma(x) = \frac{e^{x_i}}{\sum_{i=0}^{n}e^{x_i}}
$$
å…¶å¯¼æ•°å¦‚ä¸‹:
$$
\sigma'(x) = \sigma(x)(1-\sigma(x))\\
= \frac{e^{x_i}}{\sum_{i=0}^{n}e^{x_i}}(1-\frac{e^{x_i}}{\sum_{i=0}^{n}e^{x_i}})\\
$$
> ä¹Ÿè®¸æœ‰äººè§‰å¾—äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»çš„sigmoidå‡½æ•°ä¸ä¸€æ ·, ä½†å…¶å®žæ˜¯ä¸€æ ·çš„. æ¨¡åž‹ä¸Šæ˜¯ç­‰ä»·çš„. å½¢å¼ä¼šæœ‰ä¸€ç‚¹åŒºåˆ«.

## 2.4 äº¤å‰ç†µçš„å¯¼æ•°

**è¯·ä¸€å®šç†Ÿæ‚‰çŸ©é˜µä¹˜æ³•, çŸ©é˜µæ±‚å¯¼ç­‰ç­‰. å¹¶æ¸…æ™°è‡ªå·±æ¯ä¸€æ­¥æ“ä½œå¾—åˆ°çš„çŸ©é˜µå¤§å°æ˜¯å¤šå°‘. ä¸ç„¶å°±ç­‰ç€ðŸ–ðŸ§ è¿‡è½½å§**

### é“¾å¼æ³•åˆ™

> æˆ‘è§‰å¾—å„ä½ä¸è‡³äºŽè¿žé“¾å¼æ³•åˆ™éƒ½ä¸çŸ¥é“, ä½†è¿˜æ˜¯æä¸€ä¸‹å§.

å¯¹äºŽä¸€ä¸ªå¤åˆå‡½æ•°$L = CE(\sigma(z(x)))$, å…¶ä¸­$CE$æ˜¯äº¤å‰ç†µ, $\sigma$æ˜¯sigmoidå‡½æ•°, $z$æ˜¯çº¿æ€§æ¨¡åž‹, $x$æ˜¯è¾“å…¥. æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é“¾å¼æ³•åˆ™æ±‚å¯¼:
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial \sigma}\frac{\partial \sigma}{\partial z}\frac{\partial z}{\partial x}
$$
å¯¹äºŽé€»è¾‘æ–¯è’‚æ¨¡åž‹, æˆ‘ä»¬åªéœ€è¦åˆ†åˆ«è®¡ç®—$\frac{\partial L}{\partial \sigma}$, $\frac{\partial \sigma}{\partial z}$, $\frac{\partial z}{\partial x}$å³å¯.

### äº¤å‰ç†µçš„å¯¼æ•°

#### $$\frac{\partial L}{\partial \sigma}$$

äºŒåˆ†ç±»:
$$
\begin{aligned}
\frac{\partial L}{\partial \sigma} &= \frac{\partial}{\partial \sigma}(-ylog(\sigma) - (1-y)log(1-\sigma))\\
&= -\frac{y}{\sigma} + \frac{1-y}{1-\sigma}
\end{aligned}
$$
å¤šåˆ†ç±»:
$$
\begin{aligned}
\frac{\partial L}{\partial \sigma} &= \frac{\partial}{\partial \sigma}(-\sum_{i=0}^{C}y_ilog(\sigma_i))\\
\frac{\partial L}{\partial \sigma_i} &= -\frac{y_i}{\sigma_i}
\end{aligned}
$$
è¿™é‡Œ$\sigma$æ˜¯ä¸€ä¸ªå‘é‡, $\sigma_i$æ˜¯å‘é‡çš„ç¬¬$i$ä¸ªå…ƒç´ .

#### $$\frac{\partial \sigma}{\partial z}$$

äºŒåˆ†ç±»:
$$
\begin{aligned}
\frac{\partial \sigma}{\partial z} &= \frac{\partial}{\partial z}\frac{1}{1+e^{-z}}\\
&= \frac{e^{-z}}{(1+e^{-z})^2}\\
&= \frac{1}{1+e^{-z}}\frac{e^{-z}}{1+e^{-z}}\\
&= \sigma(1-\sigma)
\end{aligned}
$$

å¤šåˆ†ç±»:(ä¸ä»”ç»†å†™äº†, åªæ˜¯éº»çƒ¦ç‚¹, å¯ä»¥çœ‹è¿™é‡Œ: https://zhuanlan.zhihu.com/p/86787709)
$$
\begin{aligned}
\frac{\partial \sigma}{\partial z} &= \frac{\partial}{\partial z}\frac{e^{z_i}}{\sum_{i=0}^{n}e^{z_i}}\\
&= \frac{e^{z_i}\sum_{i=0}^{n}e^{z_i} - e^{z_i}e^{z_i}}{(\sum_{i=0}^{n}e^{z_i})^2}\\
&= \frac{e^{z_i}}{\sum_{i=0}^{n}e^{z_i}}(1-\frac{e^{z_i}}{\sum_{i=0}^{n}e^{z_i}})\\
&= \sigma(1-\sigma)
\end{aligned}
$$

#### $$\frac{\partial z}{\partial x}$$

è¿™é‡Œçš„$z$æ˜¯çº¿æ€§æ¨¡åž‹çš„è¾“å‡º, ä¹Ÿå°±æ˜¯$z = Wx + b$, å…¶ä¸­$W$æ˜¯æƒé‡çŸ©é˜µ, $b$æ˜¯åç½®å‘é‡. å› æ­¤:
$$
\frac{\partial z}{\partial x} = W
$$

### æ¢¯åº¦ä¸‹é™

å¸Œæœ›ä½ ä»¬æ²¡æœ‰å¿˜è®°æ¢¯åº¦æ˜¯ä»€ä¹ˆ. æ¢¯åº¦çš„æ–¹å‘æ˜¯å‡½æ•°ä¸Šå‡çš„æ–¹å‘.

å¯¹äºŽå¤§éƒ¨åˆ†é—®é¢˜, ä¼˜åŒ–æŸå¤±å‡½æ•°æ²¡æœ‰æ˜¾å¼è§£(close form solution), å› æ­¤æˆ‘ä»¬éœ€è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥ä¼˜åŒ–æŸå¤±å‡½æ•°. æ¢¯åº¦ä¸‹é™çš„å…¬å¼å¦‚ä¸‹:

$$
\theta_{t+1} = \theta_t - \alpha \nabla L(\theta_t)
$$
å…¶ä¸­$\theta$æ˜¯æ¨¡åž‹çš„å‚æ•°, $L$æ˜¯æŸå¤±å‡½æ•°, $\alpha$æ˜¯å­¦ä¹ çŽ‡, æœ‰æ—¶å€™ä¹Ÿå†™ä½œ$\eta$. æ³¨æ„è´Ÿå·, å› ä¸ºæˆ‘ä»¬å¸Œæœ›æ¢¯åº¦ä¸‹é™, è€Œä¸æ˜¯æ¢¯åº¦ä¸Šå‡. $\nabla L(\theta_t)$æ˜¯æŸå¤±å‡½æ•°åœ¨$\theta_t$å¤„çš„æ¢¯åº¦.

æˆ‘ä»¬æœ‰å¾ˆå¤šç§æ–¹æ³•ä¼°è®¡$\nabla L(\theta_t)$, ä»Žä½¿ç”¨çš„æ•°æ®é‡çš„ä¸åŒ, åˆ†ä¸ºä¸‰ç±»:
- Batch Gradient Descent: ä½¿ç”¨å…¨éƒ¨æ•°æ®æ¥ä¼°è®¡æ¢¯åº¦.
- Stochastic Gradient Descent: ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬æ¥ä¼°è®¡æ¢¯åº¦.
- Mini-batch Gradient Descent: ä½¿ç”¨ä¸€ä¸ªbatchçš„æ ·æœ¬æ¥ä¼°è®¡æ¢¯åº¦.

åŸºæœ¬å›´ç»•ç€ä¼°è®¡å‡†ç¡®æ€§å’Œé«˜æ•ˆæ€§çš„æƒè¡¡.

åœ¨ä»£ç ä¸­æˆ‘ä¼šä½¿ç”¨Batch Gradient Descent, ä½†æ˜¯åœ¨å®žé™…åº”ç”¨ä¸­, æˆ‘ä»¬å¾€å¾€ä½¿ç”¨Mini-batch Gradient Descent. åªè¦æ˜¯æ€•å¤ªå¤æ‚äº†æ‚¨ä»¬ä¸å¥½ç†è§£. :-)

### ç¼–ç¨‹å®žçŽ°

> è‚¯å®šæœ‰äººçœ‹æ‡‚äº†ä¸ä¼šæ‰“ä»£ç çš„, æ‰€ä»¥æˆ‘å°±å†™äº†ä¸€ä¸‹ä»£ç . 

ä½†æ˜¯æˆ‘ä¸ä¿è¯ä»£ç æ˜¯å¯¹çš„, ~~å› ä¸ºæˆ‘æ²¡è·‘è¿‡~~è·‘è¿‡äº†.è¿™é‡Œåªç”¨äºŒåˆ†ç±»åšä¾‹å­, å¤šåˆ†ç±»çš„è‡ªå·±å†™.
å…ˆé€ ä¸€ç‚¹æ•°æ®:

```python
import numpy as np
import matplotlib.pyplot as plt

# é€ ä¸€ç‚¹æ•°æ®
X = (np.random.rand(1000, 2)-0.5)*5 # (n, p)
y = (-X[:, 0]**2 + X[:, 1]**2 > 0.5) * 1 # (n, 1)

# æž„é€ ä¸€äº›ç‰¹å¾ï¼šx1^2, x2^2, x1, x2, 1
X = np.concatenate([X, np.ones((X.shape[0], 1)), X**2], axis=1) # (n, 6)

# ç»˜åˆ¶ä¸€ä¸‹æ•°æ®
plt.figure(figsize=(10, 10), dpi=100)
plt.title("Data")
plt.xlabel("x1")
plt.ylabel("x2")
plt.scatter(X[y==0, 0], X[y==0, 1], c='r', label="y=0")
plt.scatter(X[y==1, 0], X[y==1, 1], c='b', label="y=1")
plt.legend()
```

```python
# å®šä¹‰å„ä¸ªå‡½æ•°
def sigmoid(x):
    # x: (n, p)
    # return: (n, p)
    return 1 / (1 + np.exp(-x))

def cross_entropy(y, y_hat):
    # y: (n, 1)
    # y_hat: (n, 1)
    # return: æ ‡é‡
    return np.sum(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))

def cal_z(x, W):
    # x: (n, p)
    # W: (p, 1) äºŒåˆ†ç±»é—®é¢˜, å¯¹äºŽå¤šåˆ†ç±»é—®é¢˜, Wçš„ç»´åº¦æ˜¯(p, C)
    return np.dot(x, W) # (n, 1)

def cal_loss(x, y, W):
    y_hat = sigmoid(cal_z(x, W))
    return cross_entropy(y, y_hat)

def cal_grad(x, y, W):
    # x: (n, p)
    # y: (n, 1)
    # W: (p, 1)
    # return: (p, 1)

    z = cal_z(x, W)
    y_hat = sigmoid(z)
    y = y.reshape(-1, 1) # (n, 1)

    dL_dsigma = -y / y_hat + (1 - y) / (1 - y_hat) # (n, 1)
    dsigma_dz = y_hat * (1 - y_hat) # (n, 1)
    dz_dw = x # (n, p)

    grad = (dL_dsigma * dsigma_dz).T.dot(dz_dw) # (1, n) * (n, p) = (1, p)
    grad = grad.T # (p, 1)
    grad /= x.shape[0] # (p, 1)

    return grad

def accuracy(x, y, W):
    # æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ¥è®¡ç®—å‡†ç¡®çŽ‡
    y_hat = sigmoid(cal_z(x, W))
    y_hat = y_hat.reshape(-1)
    y_hat = y_hat > 0.5
    y_hat = y_hat * 1
    # è¿™é‡Œ0.5æ˜¯ä¸€ä¸ªé˜ˆå€¼, å¯ä»¥è‡ªå·±è°ƒæ•´
    return np.mean(y_hat == y)

```

```python

# å‚æ•°åˆå§‹åŒ–
## äºŒåˆ†ç±»é—®é¢˜
train_size = 1000
num_features = 5
num_classes = 1

## model parameters initialization
W = np.random.randn(num_features, num_classes) # (p, 1)


# è¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æŸå¤±å‡½æ•°

iterations = 500
learning_rate = 1e-1 # å­¦ä¹ çŽ‡, å¯ä»¥è‡ªå·±è°ƒæ•´. å¯¹äºŽä¸åŒé—®é¢˜, å­¦ä¹ çŽ‡çš„é€‰æ‹©æ˜¯ä¸åŒçš„.

# ç»˜å›¾å‚æ•°
plt.figure(figsize=(10, 6), dpi=100)
plt.title("Accuracy vs Iterations")
plt.xlabel("Iterations")
plt.ylabel("Accuracy")

accs = []
## 50æ¬¡è¿­ä»£ç»˜åˆ¶ä¸€æ¬¡å›¾
iters_obs = range(0, iterations, 10)


for i in range(iterations):
    grad = cal_grad(X, y, W)
    W -= learning_rate * grad
    loss = cal_loss(X, y, W)
    if i in iters_obs:
        acc = accuracy(X, y, W)
        accs.append(acc)
        # print("Iteration: {}, Loss: {}, Accuracy: {}".format(i, loss, acc))

# ç»˜å›¾
plt.plot(iters_obs, accs)
plt.show()

# ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
plt.subplots(figsize=(10, 5), dpi=100)
plt.subplot(1, 2, 1)
plt.title("Decision Boundary of Model")
plt.xlabel("x1")
plt.ylabel("x2")
y_hat = sigmoid(cal_z(X, W))
label_pred = y_hat > 0.5
label_pred = label_pred.reshape(-1)
plt.scatter(X[label_pred==0, 0], X[label_pred==0, 1], c='r', label="y=0")
plt.scatter(X[label_pred==1, 0], X[label_pred==1, 1], c='b', label="y=1")
plt.legend()

plt.subplot(1, 2, 2)
plt.title("Ground Truth")
plt.xlabel("x1")
plt.ylabel("x2")
plt.scatter(X[y==0, 0], X[y==0, 1], c='r', label="y=0")
plt.scatter(X[y==1, 0], X[y==1, 1], c='b', label="y=1")
plt.legend()
plt.show()
```
#### <img src="image.png" alt="Alt text" style="zoom: 50%;" />

<img src="image-1.png" alt="Alt text" style="zoom: 50%;" />

