{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg8OMZQ5vu6J"
   },
   "source": [
    "###Neural Networks in Numpy\n",
    "\n",
    "We will design and train a 2 layer Neural Network with Numpy and use it classify MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0r_ygPkBTdKu",
    "ExecuteTime": {
     "end_time": "2023-08-16T15:42:28.269640700Z",
     "start_time": "2023-08-16T15:42:21.318014400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#PyTorch Libraries\n",
    "import torch\n",
    "from torchvision import models,transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline\n",
    "\n",
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3CuAyZnssR7"
   },
   "source": [
    "Helper function for loading data and creating dataLoaders for MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BmUFRZICVpJY",
    "ExecuteTime": {
     "end_time": "2023-08-16T15:42:30.059598100Z",
     "start_time": "2023-08-16T15:42:29.991455700Z"
    }
   },
   "outputs": [],
   "source": [
    "# MNIST\n",
    "def mnist(batch_sz, valid_size=0, shuffle=True, random_seed=2000):\n",
    "    num_classes = 10\n",
    "    transform_train = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "    transform_valid = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    # Training dataset\n",
    "    train_data = MNIST(root='./datasets', train=True, download=True, transform=transform_train)\n",
    "    valid_data = MNIST(root='./datasets', train=True, download=True, transform=transform_valid)\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    if shuffle == True:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_sz, sampler=train_sampler,pin_memory=True, shuffle=False)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_sz, sampler=valid_sampler,pin_memory=True, shuffle=False)\n",
    "\n",
    "    # Test dataset\n",
    "    test_data = MNIST(root='./datasets', train=False, download=True, transform=transform_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_sz, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o4-i2FVs18y"
   },
   "source": [
    "creating train, val and test dataloaders for MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cU08-68JVuT_",
    "ExecuteTime": {
     "end_time": "2023-08-16T15:42:38.679585900Z",
     "start_time": "2023-08-16T15:42:38.481548800Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_sz=64 # this is batch size i.e. the number of rows in a batch of data\n",
    "train_loader, val_loader, test_loader=mnist(batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgRKNEd_m3OR"
   },
   "source": [
    "Training and networks parameters and hyperparamters and network initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrEV5Snamb2S"
   },
   "source": [
    "We will implement the forward pass and backward  propagation equations that we derived in the lecture on Day 1. Variable \"m\" in the back propagation equation is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[0].shape)\n",
    "\n",
    "for i in range(1):\n",
    "  plt.figure()\n",
    "  plt.imshow(batch[0][i,0,:,:],cmap='gray')\n",
    "  plt.title(batch[1][i].item())\n",
    "\n",
    "print(len(batch[0]))\n",
    "batch[1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVCcNhYBgtge",
    "outputId": "23adee88-687b-4f83-8c29-3e511377d8a1",
    "ExecuteTime": {
     "end_time": "2023-08-16T15:42:46.605430700Z",
     "start_time": "2023-08-16T15:42:45.286528400Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "64\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([7, 2, 4, 8, 6, 6, 9, 7, 5, 9, 9, 0, 8, 6, 2, 1, 7, 1, 5, 8, 0, 3, 1, 8,\n        2, 7, 0, 2, 2, 9, 4, 8, 9, 3, 3, 4, 5, 3, 2, 2, 3, 8, 5, 3, 5, 6, 1, 9,\n        7, 1, 2, 5, 0, 7, 0, 0, 3, 9, 1, 5, 5, 0, 9, 9])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u_e7ldrtHh1"
   },
   "source": [
    "The main training loop, with batch gradient decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    X input size: (batch, p)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(in_features, out_features)\n",
    "        self.b = np.random.randn(out_features)\n",
    "        # forward: Z (batch, out_features) <- X (batch, in_features) * W (in_features, out_features) + b (out_features)\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "        self.X = None\n",
    "        self.batchSize = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.batchSize = X.shape[0]\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # dZ (batch, out_features)\n",
    "        # dW (in_features, out_f) <- X.T (in_features, batch) * dZ (batch, out_features)\n",
    "        self.grad_W = np.dot(self.X.T, dZ)\n",
    "        self.grad_b = np.sum(dZ, axis=0)\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.W -= lr * self.grad_W\n",
    "        self.b -= lr * self.grad_b\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = np.maximum(X, 0)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dX = dZ * (self.Z > 0)\n",
    "        return dX\n",
    "\n",
    "class sigmoid:\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = 1/(1+np.exp(-X))\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dX = dZ * self.Z * (1-self.Z)\n",
    "        return dX\n",
    "\n",
    "class MSE:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        self.y = y\n",
    "        self.y_hat = y_hat\n",
    "        return np.mean((y - y_hat)**2)\n",
    "\n",
    "    def backward(self):\n",
    "        return 2*(self.y_hat - self.y)/self.y.shape[0]\n",
    "\n",
    "class crossEntropy:\n",
    "    \"\"\"\n",
    "    cross entropy loss for multi-class classification, y is one-hot encoded\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_classes = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        self.y = y\n",
    "        self.y_hat = y_hat\n",
    "        self.num_classes = y.shape[1]\n",
    "        return -np.sum(y*np.log(y_hat))/y.shape[0] #expectation CrossEntropy\n",
    "\n",
    "    def backward(self):\n",
    "        return -self.y/self.y_hat/self.y.shape[0] #expectation CrossEntropy\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    \"\"\"\n",
    "    :param y: (batch, num_classes)\n",
    "    :param y_hat: (batch, num_classes)\n",
    "    \"\"\"\n",
    "    return np.mean(np.argmax(y, axis=1) == np.argmax(y_hat, axis=1), axis=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T15:48:48.118246800Z",
     "start_time": "2023-08-16T15:48:48.103289700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model = [\n",
    "    Linear(28*28, 50),\n",
    "    sigmoid(),\n",
    "    Linear(50, 10),\n",
    "    sigmoid(),\n",
    "    crossEntropy()\n",
    "]\n",
    "batch_size = 256\n",
    "lr = 1e-1\n",
    "epochs = 10\n",
    "losses = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T15:58:34.396981700Z",
     "start_time": "2023-08-16T15:58:34.375935800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.00018641529793587822\n",
      "epoch 0, training accuracy 0.09375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lifan Lin\\AppData\\Local\\Temp\\ipykernel_6080\\4151497030.py:53: RuntimeWarning: overflow encountered in exp\n",
      "  self.Z = 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, test accuracy 0.12668333333333334\n",
      "epoch 1, loss 0.00013265489260334663\n",
      "epoch 1, training accuracy 0.1875\n",
      "epoch 1, test accuracy 0.12703333333333333\n",
      "epoch 2, loss 0.0001723838249918556\n",
      "epoch 2, training accuracy 0.15625\n",
      "epoch 2, test accuracy 0.12728333333333333\n",
      "epoch 3, loss 0.0003796147358027925\n",
      "epoch 3, training accuracy 0.125\n",
      "epoch 3, test accuracy 0.12736666666666666\n",
      "epoch 4, loss 4.9270496467936695e-05\n",
      "epoch 4, training accuracy 0.125\n",
      "epoch 4, test accuracy 0.12775\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# let me try whether it works\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m X_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;66;03m# transform X_batch to (batch, 28*28)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m         X_batch \u001B[38;5;241m=\u001B[39m X_batch\u001B[38;5;241m.\u001B[39mview(X_batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;66;03m# transform y_batch to one-hot encoding\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    142\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 145\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    148\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\dataOpr\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    170\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mview(pic\u001B[38;5;241m.\u001B[39msize[\u001B[38;5;241m1\u001B[39m], pic\u001B[38;5;241m.\u001B[39msize[\u001B[38;5;241m0\u001B[39m], F_pil\u001B[38;5;241m.\u001B[39mget_image_num_channels(pic))\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[1;32m--> 172\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mdefault_float_dtype)\u001B[38;5;241m.\u001B[39mdiv(\u001B[38;5;241m255\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# let me try whether it works\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # transform X_batch to (batch, 28*28)\n",
    "        X_batch = X_batch.view(X_batch.shape[0], -1)\n",
    "        # transform y_batch to one-hot encoding\n",
    "        vec = y_batch.numpy()\n",
    "        y_batch = np.zeros((y_batch.shape[0], 10))\n",
    "        y_batch[np.arange(y_batch.shape[0]), vec] = 1\n",
    "        # forward\n",
    "        for layer in model[:-1]: # the last layer is the loss function, and it needs 2 inputs, so we don't forward it here\n",
    "            X_batch = layer.forward(X_batch)\n",
    "            # X_batch is the output of the layer, its size changes to the output size of the layer\n",
    "\n",
    "        loss = model[-1].forward(y_batch, X_batch) # here we record the loss in the last layer(loss function)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # backward\n",
    "        ## In my torch-like design, the gradient is stored in the layer itself, so we need to backward from the last layer to the first layer\n",
    "        dX = model[-1].backward()\n",
    "        # dX is not the \"gradient of X\", but the gradient of current layer's input.\n",
    "        for layer in model[-2::-1]: # from the second last layer to the first layer, because the last layer is the loss function\n",
    "            dX = layer.backward(dX)\n",
    "\n",
    "        ## after backward, the gradient is stored in the layer itself, so we can update the parameters\n",
    "\n",
    "        # update\n",
    "        ## should be just simple addition.\n",
    "        for layer in model:\n",
    "            # If it has func update, then update\n",
    "            if hasattr(layer, \"update\"): # this looks like a powerful function\n",
    "                layer.update(lr)\n",
    "    print(f\"epoch {epoch}, loss {loss}\")\n",
    "    print(f\"epoch {epoch}, training accuracy {accuracy(y_batch, X_batch)}\")\n",
    "    # evaluate on test set\n",
    "    X_test = val_loader.dataset.data.view(val_loader.dataset.data.shape[0], -1).numpy()\n",
    "    y_test = np.zeros((val_loader.dataset.targets.shape[0], 10))\n",
    "    y_test[np.arange(y_test.shape[0]), val_loader.dataset.targets.numpy()] = 1\n",
    "    for layer in model[:-1]:\n",
    "        X_test = layer.forward(X_test)\n",
    "    print(f\"epoch {epoch}, test accuracy {accuracy(y_test, X_test)}\")\n",
    "\n",
    "# plot loss\n",
    "plt.plot(losses)\n",
    "plt.title(\"loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T16:05:52.576685200Z",
     "start_time": "2023-08-16T16:04:55.015466Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_z(X, W, b):\n",
    "  return W@X+b\n",
    "\n",
    "def sigmoid(Z):\n",
    "  return 1/(1+np.exp(-Z))\n",
    "\n",
    "def loss(yhat,y):\n",
    "  l = np.mean (np.sum(-y*np.log(yhat), axis = 0))\n",
    "  return l"
   ],
   "metadata": {
    "id": "322w1n_LiZ39"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ni = 28*28 # no of input feature\n",
    "nh = 50   # no of hidden units/nodes\n",
    "no = 10   #no of out channel (has to be equal ot the number of classes)\n",
    "\n",
    "W1 = np.random.randn(nh,ni) # the number of rows of w1 has to equal teh of hidden untis na dhte numbero fcolumsn have ot equal the numebrof rows of X\n",
    "b1 = np.zeros((nh , 1)) # b1 is a column vector and the rows have to match z1\n",
    "\n",
    "W2 = np.random.randn(no ,nh) # the number of rows have to be equal to out channels nad the no of columsn eaul to the number of rows of A1\n",
    "b2 = np.zeros((no, 1)) # column vector, rows equal to z2\n",
    "\n",
    "num_epochs = 8\n",
    "lr = 0.1\n",
    "\n",
    "# X [x_1, x_2, x_3, ...  x_N]\n",
    "ls = []\n",
    "for i in range(num_epochs):\n",
    "  error = 0\n",
    "  num_images = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  total_loss = 0\n",
    "  for batch in train_loader:\n",
    "    X = batch[0]\n",
    "    labels = batch[1]\n",
    "    X = batch[0].numpy().reshape(-1,28*28).T\n",
    "    y = np.zeros((no, X.shape[1]))\n",
    "    y[labels, np.arange(y.shape[1])] = 1\n",
    "\n",
    "    # y[[2, 9,4,7],[0,1,2,3]]\n",
    "    #y[2,0]=1\n",
    "    #y[9,1] =1\n",
    "    #y[4,2]=1\n",
    "    #y[7,3] =1\n",
    "\n",
    "    #forward pass\n",
    "    Z1 = W1@X + b1 ##nn.Linear (layer 1)\n",
    "    A1 = sigmoid(Z1) #torch.sigmoid\n",
    "\n",
    "    Z2 = W2@A1 + b2  ##nn.Linear (layer 2)\n",
    "    yhat = sigmoid(Z2) #\n",
    "\n",
    "    #backward pass\n",
    "    dZ2 = yhat - y\n",
    "\n",
    "    dW2 = dZ2 @ A1.T\n",
    "    db2 = np.sum(dZ2, axis =1 , keepdims = True)\n",
    "\n",
    "    dZ1 = W2.T @ dZ2 * A1*(1-A1)\n",
    "\n",
    "    dW1 = dZ1 @ X.T\n",
    "    db1 = np.sum(dZ1, axis =1 , keepdims = True)\n",
    "\n",
    "\n",
    "    #optimzation step\n",
    "    #print(np.max(W1), np.max(dW1), np.max(W2), np.max(dW2))\n",
    "    # optimization step\n",
    "    W2 -= lr*dW2\n",
    "    b2 -= lr*db2\n",
    "    W1 -= lr*dW1\n",
    "    b1 -= lr*db1\n",
    "\n",
    "\n",
    "\n",
    "    #calcuating classification error\n",
    "    num_images += X.shape[1]\n",
    "    error += np.sum(( np.argmax(yhat, axis = 0)!= labels.numpy())*1)\n",
    "    #calcuating classification error\n",
    "    pred = np.argmax(yhat, axis = 0)\n",
    "    correct+= np.sum(labels.numpy() == pred)\n",
    "    total+=yhat.shape[1]\n",
    "\n",
    "  print(f\"Accuray is {correct/total}\")\n",
    "  print(f\"{error} out of {num_images} are incorreclty labeled\")\n",
    "\n",
    "\n",
    "plt.plot(ls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPau6DPohn4Z",
    "outputId": "ad178899-ebac-42dc-8c56-79a4f3c721db"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuray is 0.8276333333333333\n",
      "10342 out of 60000 are incorreclty labeled\n",
      "Accuray is 0.9227\n",
      "4638 out of 60000 are incorreclty labeled\n",
      "Accuray is 0.9338666666666666\n",
      "3968 out of 60000 are incorreclty labeled\n",
      "Accuray is 0.9406166666666667\n",
      "3563 out of 60000 are incorreclty labeled\n",
      "Accuray is 0.94455\n",
      "3327 out of 60000 are incorreclty labeled\n",
      "Accuray is 0.9479\n",
      "3126 out of 60000 are incorreclty labeled\n",
      "Accuray is 0.9503833333333334\n",
      "2977 out of 60000 are incorreclty labeled\n",
      "Accuray is 0.9509833333333333\n",
      "2941 out of 60000 are incorreclty labeled\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f05c62f4880>]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#yhat ---> column 1 (1 data point) ---[[0],[0.1],[0],[0.5],[0.3],[0]...]\n",
    "# y ---> colum 1 (1 data point)----> 2 --->[[0],[0],[1],[0],[0],...] --> one hot encoding\n"
   ],
   "metadata": {
    "id": "ksMutMN8kh1V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {
    "id": "4z2teb3Nvemf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels==y"
   ],
   "metadata": {
    "id": "Trymwh9EvhzM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(X[:,0].reshape(28,28))\n"
   ],
   "metadata": {
    "id": "ntdTFfkhnlYf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dW2"
   ],
   "metadata": {
    "id": "CVmYwvaoqDzc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Jtaq-nooqSj9"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
